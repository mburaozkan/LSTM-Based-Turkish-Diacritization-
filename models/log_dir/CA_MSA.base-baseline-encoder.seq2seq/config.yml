session_name: base-baseline-encoder
data_directory: data
data_type: CA_MSA
log_directory: log_dir
load_training_data: true
load_test_data: false
load_validation_data: true
n_training_examples:
n_test_examples:
n_validation_examples:
test_file_name: test.csv
is_data_preprocessed: false
data_separator: '|'
diacritics_separator: '*'
text_encoder: ArabicEncoderWithStartSymbol
text_cleaner: valid_arabic_cleaners
max_len: 600
max_steps: 2_000_000
learning_rate: 0.001
batch_size: 16
adam_beta1: 0.9
adam_beta2: 0.999
use_decay: true
weight_decay: 0.0
encoder_embedding_dim: 256
decoder_embedding_dim: 256
encoder_dim: 512
encoder_units: [256, 256, 256]
use_batch_norm: true
decoder_units: 256
decoder_layers: 2
attention_units: 256
use_decoder_prenet: true
teacher_forcing_probability: 0.0
decoder_prenet_depth: [256, 128]
is_attention_accumulative: true
attention_type: LocationSensitive
use_mixed_precision: false
optimizer_type: Adam
device: cuda
evaluate_frequency: 5000
evaluate_with_error_rates_frequency: 5000
n_predicted_text_tensorboard: 10
model_save_frequency: 5000
train_plotting_frequency: 1000
n_steps_avg_losses: [100, 500, 1_000, 5_000] # command line display of average loss values for the last n steps
error_rates_n_batches: 10000
test_model_path:
train_resume_model_path:
len_input_symbols: 44
len_target_symbols: 17
optimizer: Adam
git_hash: a71b578
